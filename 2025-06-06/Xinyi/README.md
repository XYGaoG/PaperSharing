## When Do LLMs Help With Node Classification? A Comprehensive Analysis

### Why did you choose to share this paper?
  
I shared this paper because it presents the first comprehensive evaluation and comparison of LLM-based and GNN-based methods for node classification. It covers 10 homophilic datasets, 4 heterophilic datasets, 8 LLM-based algorithms, 8 classical baselines, and 3 learning paradigms (semi-supervised, supervised, and zero-shot learning). Through extensive experiments, the authors provide several important insights into the use of LLMs for graph learning, offering valuable guidance for future research.

### What is the motivation behind this paper?

LLMs have recently been introduced into graph learning to enhance node classification performance. However, their integration follows diverse paradigms. In supervised settings, LLMs can serve as encoders, explainers, or predictors. For zero-shot learning, they are used for direct inference or as components of graph foundation models. Despite their growing use, the performance of LLM-based methods relative to traditional GNNs remains unclear. This paper aims to clarify the effectiveness of each paradigm across various experimental settings.

### How do you envision applying the methods from this paper to your own research?
  
This paper offers several key insights into the use of LLMs for graph data:

- **LLM-based approaches** provide only marginal improvements over traditional methods when ample supervision is available.  
- LLM-based approaches show greater advantages in semi-supervised settings.  
- The **LLM-as-Predictor** paradigm requires a large number of labeled examples to perform well.  
- **Graph Foundation Models (GFMs)** slightly outperform open-source LLMs but still fall short of SOTA LLM like **GPT-4o**, which achieves the best performance. This highlights the need for further research on GFMs.  
- The performance of **direct LLM inference** can be significantly improved by incorporating **structural information** from the graph.  
- The **LLM-as-Encoder** paradigm performs especially well on graphs with **low heterophily**, clearly outperforming traditional language models.

### How can the ideas in this paper be generalized to other areas or problems?

The paradigms of LLM usage in the graph domain are also commonly observed in recommendation systems and multi-modal learning. The insights related to label quantity and data heterophily can inform the design of more effective LLM based methods across these fields.
